{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xujiantong/miniconda3/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/xujiantong/miniconda3/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/xujiantong/miniconda3/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868b92c51f544c798acbd90a8def22c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "selected_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
    "# - Generate human readable output, avoid creating output with gibberish text.\n",
    "# - Generate only the requested output, don't include any other language before or after the requested output.\n",
    "# - Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
    "# - Generate professional language typically used in business documents in North America.\n",
    "# - Never generate offensive or foul language.\n",
    "# \"\"\"\n",
    "\n",
    "# query_wrapper_prompt = PromptTemplate(\n",
    "#     \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n\"\n",
    "#     \"Context information is below.\\\\n\"\n",
    "#     \"---------------------\\\\n\"\n",
    "#     \"{context_str}\\\\n\"\n",
    "#     \"---------------------\\\\n\"\n",
    "#     \"Given the context information and not prior knowledge, \"\n",
    "#     \"answer the query. Please be brief, concise, and complete.\\\\n\"\n",
    "#     \"If the context information does not contain an answer to the query, \"\n",
    "#     \"respond with \\\"No information\\\".\"\n",
    "#     \"Query: {query_str}\\\\n\"\n",
    "#     \"Answer: \"\n",
    "# )\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=30000,\n",
    "    max_new_tokens=2048,\n",
    "    # query_wrapper_prompt=query_wrapper_prompt,\n",
    "    model_name=selected_model,\n",
    "    tokenizer_name=selected_model,\n",
    "    device_map=\"mps\",\n",
    "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": False, \"trust_remote_code\":True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/xujiantong/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-en and are newly initialized: ['encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "# Replace 'your-huggingface-model' with the actual model name from Hugging Face\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"jinaai/jina-embeddings-v2-base-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import pypdf \n",
    "import pandas as pd \n",
    "from llama_index.evaluation import ( \n",
    "    RelevancyEvaluator, \n",
    "    FaithfulnessEvaluator, \n",
    ") \n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader, \n",
    "    VectorStoreIndex, \n",
    "    ServiceContext,\n",
    "    set_global_service_context\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../data/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    embed_model=embed_model\n",
    ")\n",
    "set_global_service_context(service_context)\n",
    "index = VectorStoreIndex.from_documents(  \n",
    "    documents=documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal agents refer to artificial intelligence systems that can process and analyze data from multiple sources or modalities, such as text, images, and videos, to understand and interact with complex environments. These agents can utilize large language models (LLMs) and vision language models (VLMs) to analyze text data, including chat logs, player feedback, and narrative content, as well as image and video data from gaming sessions. They can help identify patterns of player behavior, preferences, and interactions, facilitate the development of intelligent agents within games, and assist in the creation and enhancement of immersive gaming environments through scene synthesis.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is Multimodal Agents?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index import get_response_synthesizer\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    text_qa_template=query_wrapper_prompt,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "response = query_engine.query(\"What is Multimodal Agents?\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "token_count = 0\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")\n",
    "    token_count += 1\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "tokens_per_second = token_count / time_elapsed\n",
    "\n",
    "print(f\"\\n\\nStreamed output at {tokens_per_second} tokens/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompt import PromptTemplate\n",
    "\n",
    "# Define the prompt template string\n",
    "qa_prompt_tmpl_str = \"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {query_str}\n",
    "Answer: \\\n",
    "\"\"\"\n",
    "\n",
    "# Define a function to format the context\n",
    "def format_context_fn(**kwargs):\n",
    "    # format context with bullet points\n",
    "    context_list = kwargs[\"context_str\"].split(\"\\n\\n\")\n",
    "    fmtted_context = \"\\n\\n\".join([f\"- {c}\" for c in context_list])\n",
    "    return fmtted_context\n",
    "\n",
    "# Create a PromptTemplate instance\n",
    "prompt_tmpl = PromptTemplate(\n",
    "    qa_prompt_tmpl_str, function_mappings={\"context_str\": format_context_fn}\n",
    ")\n",
    "\n",
    "# Define the context string\n",
    "context_str = \"\"\"\\\n",
    "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
    "Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases.\n",
    "Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models.\n",
    "\"\"\"\n",
    "\n",
    "# Use the PromptTemplate instance to format the context and query\n",
    "fmt_prompt = prompt_tmpl.format(\n",
    "    context_str=context_str, query_str=\"How many params does llama 2 have\"\n",
    ")\n",
    "\n",
    "# Print the formatted prompt\n",
    "print(fmt_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
